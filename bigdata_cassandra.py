# -*- coding: utf-8 -*-
"""BIgData_cassandra

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qcfIwJLbhymHJEAPwHNP60SBIp03Sy3B
"""

!pip install cassandra-driver pandas requests  # Install required libraries

from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
import json

# This secure connect bundle is autogenerated when you download your SCB,
# if yours is different update the file name below
cloud_config= {
  'secure_connect_bundle': 'secure-connect-warehouse-sales.zip'
}

# This token JSON file is autogenerated when you download your token,
# if yours is different update the file name below
with open("warehouse_sales-token.json") as f:
    secrets = json.load(f)

CLIENT_ID = secrets["clientId"]
CLIENT_SECRET = secrets["secret"]

auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
session = cluster.connect()

row = session.execute("select release_version from system.local").one()
if row:
  print(row[0])
else:
  print("An error occurred.")

import requests
import pandas as pd

url = "https://data.montgomerycountymd.gov/api/views/v76h-r7br/rows.json?accessType=DOWNLOAD"
response = requests.get(url)
data = response.json()

# Convert to Pandas DataFrame
rows = data.get('data', [])
columns = [col['name'] for col in data['meta']['view']['columns']]
df = pd.DataFrame(rows, columns=columns)

df.columns

from cassandra.query import BatchStatement
import pandas as pd
import uuid
from datetime import datetime
import requests

session.execute(f"""
CREATE TABLE IF NOT EXISTS sales.Bronze (
    id UUID PRIMARY KEY,
    year INT,
    month INT,
    supplier TEXT,
    item_code TEXT,
    item_description TEXT,
    item_type TEXT,
    retail_sales DECIMAL,
    retail_transfers DECIMAL,
    warehouse_sales DECIMAL,
    created_at TIMESTAMP
);
""")

insert_stmt = session.prepare(f"""
INSERT INTO sales.Bronze (
    id, year, month, supplier, item_code, item_description,
    item_type, retail_sales, retail_transfers, warehouse_sales, created_at
) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
""")

batch_size = 1000
batch = BatchStatement()
inserted_count = 0

from tqdm import tqdm
def insert_rows(dataframe, total_rows, batch_size):
    inserted = 0
    with tqdm(total=total_rows, desc="Inserting rows") as pbar:
        for i in range(0, total_rows, batch_size):
            batch = BatchStatement()
            end_idx = min(i + batch_size, total_rows)

            for _, row in dataframe.iloc[i:end_idx].iterrows():
                try:
                    batch.add(insert_stmt, (
                        uuid.uuid4(),
                        int(row['YEAR']),
                        int(row['MONTH']),
                        str(row['SUPPLIER']),
                        str(row['ITEM CODE']),
                        str(row['ITEM DESCRIPTION']),
                        str(row['ITEM TYPE']),
                        float(row['RETAIL SALES']),
                        float(row['RETAIL TRANSFERS']),
                        float(row['WAREHOUSE SALES']),
                        datetime.now()
                    ))
                except Exception as e:
                    print(f"Skipping row {i} due to error: {str(e)}")
                    continue

            session.execute(batch)
            inserted += (end_idx - i)
            pbar.update(end_idx - i)

    return inserted

# Execute the insertion
success_count = insert_rows(df, 120000, batch_size)
print(f"Total rows inserted: {success_count}")

result = session.execute(f"SELECT COUNT(*) FROM sales.Bronze").one()
print(f"\nSuccessfully inserted {success_count} rows")
cluster.shutdown()

df = session.execute(f"SELECT * FROM sales.Bronze")

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from decimal import Decimal
import uuid
from datetime import datetime

# Create Spark Session
spark = SparkSession.builder \
    .appName("CassandraToSpark") \
    .getOrCreate()

# Function to properly extract data from Cassandra rows
def convert_cassandra_row(row):
    return {
        'id': str(row.id) if row.id else None,
        'created_at': row.created_at if row.created_at else None,
        'item_code': row.item_code if row.item_code else None,
        'item_description': row.item_description if row.item_description else None,
        'item_type': row.item_type if row.item_type else None,
        'month': int(row.month) if row.month else None,
        'retail_sales': float(row.retail_sales) if row.retail_sales else None,
        'retail_transfers': float(row.retail_transfers) if row.retail_transfers else None,
        'supplier': row.supplier if row.supplier else None,
        'warehouse_sales': float(row.warehouse_sales) if row.warehouse_sales else None,
        'year': int(row.year) if row.year else None
    }

# Convert all rows
data = [convert_cassandra_row(row) for row in df]

# Define schema
schema = StructType([
    StructField("id", StringType(), True),
    StructField("created_at", TimestampType(), True),
    StructField("item_code", StringType(), True),
    StructField("item_description", StringType(), True),
    StructField("item_type", StringType(), True),
    StructField("month", IntegerType(), True),
    StructField("retail_sales", DoubleType(), True),
    StructField("retail_transfers", DoubleType(), True),
    StructField("supplier", StringType(), True),
    StructField("warehouse_sales", DoubleType(), True),
    StructField("year", IntegerType(), True)
])

# Create DataFrame
spark_df = spark.createDataFrame(data, schema=schema)

# Show results
spark_df.show(5, truncate=False)

"""### Bronze Layer"""

# Cache the DataFrame for better performance
spark_df.cache()

# 1. Basic Exploration
print("Record Count:", spark_df.count())
spark_df.printSchema()
spark_df.show(5, vertical=True)

# 2. Data Quality Checks
from pyspark.sql.functions import col, count, when, isnull

# Null check per column
null_checks = spark_df.select(
    [count(when(isnull(c), c)).alias(c) for c in spark_df.columns]
)
null_checks.show()

# 3. Basic Cleaning (Bronze Layer Standards)
bronze_df = spark_df.withColumnRenamed("supplier", "supplier_name") \
    .withColumn("ingestion_timestamp", current_timestamp()) \
    .withColumn("data_source", lit("cassandra.sales.Bronze"))

"""### Silver Layer"""

from pyspark.sql.functions import col

# Remove invalid rows where 'year', 'month', or key fields are zero
silver_df = spark_df.filter(
    (col("year") != 0) &
    (col("month") != 0) &
    (col("id") != "0") &
    (col("item_code") != "0") &
    (col("item_type") != "0") &
    (col("supplier") != "0")
)
silver_df = silver_df.dropDuplicates(["id"]) \
    .filter(
        (col("retail_sales") >= 0) &
        (col("warehouse_sales") >= 0)
    ) \
    .na.fill({
        "item_description": "Unknown",
        "supplier": "Unknown"
    }) \
    .filter(col("id").isNotNull())

silver_df.show()

"""### Gold Layer"""

agg1 = silver_df.groupBy("year") \
    .sum("retail_sales") \
    .withColumnRenamed("sum(retail_sales)", "total_retail_sales")
agg1.show()

agg2 = silver_df.groupBy("year", "month", "item_code") \
    .sum("retail_sales") \
    .withColumnRenamed("sum(retail_sales)", "monthly_retail_sales")
agg2.show()

from pyspark.sql.functions import sum

agg3 = silver_df.groupBy("item_code") \
    .agg(sum("warehouse_sales").alias("total_warehouse_sales")) \
    .orderBy(col("total_warehouse_sales").desc()) \
    .limit(5)
agg3.show()

from pyspark.sql.functions import avg

agg4 = silver_df.groupBy("item_type") \
    .agg(avg("retail_transfers").alias("avg_retail_transfers"))
agg4.show()

agg5 = silver_df.groupBy("supplier") \
    .agg(
        sum("retail_sales").alias("total_retail_sales"),
        sum("warehouse_sales").alias("total_warehouse_sales")
    )
agg5.show()

agg6 = silver_df.groupBy("item_type").count().withColumnRenamed("count", "item_type_count")
agg6.show()

from pyspark.sql.window import Window
from pyspark.sql.functions import lag, round

yearly_sales = silver_df.groupBy("year") \
    .agg(sum("retail_sales").alias("total_sales"))

window_spec = Window.orderBy("year")

agg7 = yearly_sales.withColumn("prev_year_sales", lag("total_sales").over(window_spec)) \
    .withColumn("growth_percentage", round(
        ((col("total_sales") - col("prev_year_sales")) / col("prev_year_sales")) * 100, 2
    ))
agg7.show()

"""### Visualizations"""

import matplotlib.pyplot as plt
import seaborn as sns

df1 = agg1.toPandas().sort_values("year")

plt.figure(figsize=(8, 5))
sns.lineplot(data=df1, x="year", y="total_retail_sales", marker="o")
plt.title("Total Retail Sales per Year")
plt.xlabel("Year")
plt.ylabel("Retail Sales")
plt.grid(True)
plt.tight_layout()
plt.show()

df3 = agg3.toPandas()

plt.figure(figsize=(8, 5))
sns.barplot(data=df3, x="item_code", y="total_warehouse_sales", palette="viridis")
plt.title("Top 5 Items by Warehouse Sales")
plt.xlabel("Item Code")
plt.ylabel("Warehouse Sales")
plt.tight_layout()
plt.show()

df4 = agg4.toPandas()

plt.figure(figsize=(8, 5))
sns.barplot(data=df4, x="avg_retail_transfers", y="item_type", palette="magma")
plt.title("Average Retail Transfers per Item Type")
plt.xlabel("Avg Retail Transfers")
plt.ylabel("Item Type")
plt.tight_layout()
plt.show()

df5 = agg5.toPandas().sort_values("total_retail_sales", ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=df5, x="supplier", y="total_retail_sales", color="steelblue", label="Retail Sales")
sns.barplot(data=df5, x="supplier", y="total_warehouse_sales", color="orange", label="Warehouse Sales", alpha=0.7)
plt.title("Retail vs Warehouse Sales per Supplier")
plt.xlabel("Supplier")
plt.ylabel("Sales")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df6 = agg6.toPandas()

plt.figure(figsize=(8, 5))
sns.barplot(data=df6, x="item_type", y="item_type_count", palette="coolwarm")
plt.title("Item Type Distribution")
plt.xlabel("Item Type")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

df7 = agg7.toPandas().sort_values("year")

plt.figure(figsize=(8, 5))
sns.lineplot(data=df7, x="year", y="growth_percentage", marker="o", color="green")
plt.title("Year-over-Year Retail Sales Growth (%)")
plt.xlabel("Year")
plt.ylabel("Growth %")
plt.axhline(0, color='gray', linestyle='--')
plt.tight_layout()
plt.show()

from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType

# Select and cast required columns
ml_df = silver_df.select(
    col("retail_sales").cast(DoubleType()),
    col("warehouse_sales").cast(DoubleType()),
    col("retail_transfers").cast(DoubleType()),
    col("month").cast(DoubleType()),
    col("year").cast(DoubleType())
).dropna()

from pyspark.ml.feature import VectorAssembler

# Assemble features
assembler = VectorAssembler(
    inputCols=["warehouse_sales", "retail_transfers", "month", "year"],
    outputCol="features"
)

data = assembler.transform(ml_df).select("features", "retail_sales")

from pyspark.ml.regression import LinearRegression

# Train/Test split
train, test = data.randomSplit([0.8, 0.2], seed=42)

# Model
lr = LinearRegression(featuresCol="features", labelCol="retail_sales")
model = lr.fit(train)

# Predict
predictions = model.transform(test)
predictions.select("retail_sales", "prediction").show(5)

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(labelCol="retail_sales", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)

print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

